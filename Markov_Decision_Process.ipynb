{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIdsrGrYDXDr7ScGPsPDg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishnav2804/Reinforcement-Learning-Notebooks/blob/main/Markov_Decision_Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MDP\n",
        "**A Finite Markov Decision Process (MDP)** is a framework used in Reinforcement Learning to model decision-making problems where outcomes are partly random and partly under the control of a decision maker (agent). It provides a mathematical model for situations where you need to choose an action in some state to maximize the expected reward over time.\n",
        "\n",
        "**Key Concepts of MDP:**\n",
        "States (S): A finite set of all possible situations the agent can be in.\n",
        "Actions (A): A finite set of all possible actions the agent can take.\n",
        "Transition Probability (P): Probability of moving from one state to another, given an action.\n",
        "Rewards (R): Immediate reward received after transitioning from one state to another.\n",
        "Discount Factor (γ): How much future rewards are valued compared to immediate rewards (between 0 and 1)."
      ],
      "metadata": {
        "id": "pvW2YdA4lBoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Gridworld\n",
        "\n",
        "Let’s assume a simple grid where an agent starts at the top-left corner and wants to reach the bottom-right corner. It can move UP, DOWN, LEFT, or RIGHT. The agent receives a reward of +1 when it reaches the goal, and -1 for hitting a wall or stepping outside the grid. There are penalties for bad moves.\n",
        "\n",
        "Let’s break this down into an MDP:\n",
        "\n",
        "States (S): Grid cells.\n",
        "\n",
        "Actions (A): {UP, DOWN, LEFT, RIGHT}.\n",
        "\n",
        "Rewards (R): +1 for reaching the goal, -1 for hitting walls.\n",
        "\n",
        "Transition (P): Probability of moving to the next cell based on an action (we can assume deterministic moves for simplicity)."
      ],
      "metadata": {
        "id": "xqA2oo-ClHM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gridworld setup\n",
        "class GridworldMDP:\n",
        "    def __init__(self, grid_size):\n",
        "        self.grid_size = grid_size\n",
        "        self.state = (0, 0)  # Start state at the top-left corner (0, 0)\n",
        "        self.goal_state = (grid_size - 1, grid_size - 1) # goal_state represents the position in the grid where the agent is trying to reach. Specifically, it's the bottom-right corner of the grid.\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def get_actions(self):\n",
        "        return self.actions\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = self.state\n",
        "\n",
        "        # Determine new state based on action\n",
        "        if action == 'UP':\n",
        "            new_state = (max(row - 1, 0), col)\n",
        "        elif action == 'DOWN':\n",
        "            new_state = (min(row + 1, self.grid_size - 1), col)\n",
        "        elif action == 'LEFT':\n",
        "            new_state = (row, max(col - 1, 0))\n",
        "        elif action == 'RIGHT':\n",
        "            new_state = (row, min(col + 1, self.grid_size - 1))\n",
        "\n",
        "        # Reward structure\n",
        "        if new_state == self.goal_state:\n",
        "            reward = 1  # Reached the goal\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.1  # Small penalty for each move\n",
        "            done = False\n",
        "\n",
        "        self.state = new_state\n",
        "        return new_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "GE8id67DlFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating the agent interaction with the MDP\n",
        "gridworld = GridworldMDP(grid_size=4)\n",
        "state = gridworld.reset()\n",
        "\n",
        "for _ in range(500):  # Let's simulate for 20 steps\n",
        "    action = np.random.choice(gridworld.get_actions())  # Randomly choose an action\n",
        "    next_state, reward, done = gridworld.step(action)\n",
        "    print(f\"Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
        "\n",
        "    if done:\n",
        "        print(\"Reached the goal!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5xfINkPlTai",
        "outputId": "65db0424-1504-45d3-949f-4c63a6fcc661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: UP, Next State: (0, 0), Reward: -0.1\n",
            "Action: LEFT, Next State: (0, 0), Reward: -0.1\n",
            "Action: DOWN, Next State: (1, 0), Reward: -0.1\n",
            "Action: DOWN, Next State: (2, 0), Reward: -0.1\n",
            "Action: RIGHT, Next State: (2, 1), Reward: -0.1\n",
            "Action: DOWN, Next State: (3, 1), Reward: -0.1\n",
            "Action: UP, Next State: (2, 1), Reward: -0.1\n",
            "Action: RIGHT, Next State: (2, 2), Reward: -0.1\n",
            "Action: LEFT, Next State: (2, 1), Reward: -0.1\n",
            "Action: RIGHT, Next State: (2, 2), Reward: -0.1\n",
            "Action: LEFT, Next State: (2, 1), Reward: -0.1\n",
            "Action: RIGHT, Next State: (2, 2), Reward: -0.1\n",
            "Action: LEFT, Next State: (2, 1), Reward: -0.1\n",
            "Action: RIGHT, Next State: (2, 2), Reward: -0.1\n",
            "Action: DOWN, Next State: (3, 2), Reward: -0.1\n",
            "Action: DOWN, Next State: (3, 2), Reward: -0.1\n",
            "Action: RIGHT, Next State: (3, 3), Reward: 1\n",
            "Reached the goal!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Explanation:\n",
        "\n",
        "In reinforcement learning (RL), if an environment has the Markov property, it means that the future state of the environment depends only on the current state and the action taken, not on the sequence of events that preceded it.\n",
        "\n",
        "This is a crucial property because it simplifies the decision-making process for the agent. Instead of having to consider the entire history of states and actions, the agent can focus solely on the current situation to predict the potential outcomes of its actions.\n",
        "\n",
        "To put it more formally:\n",
        "\n",
        "```P(s_(t+1) | s_t, a_t) = P(s_(t+1) | s_1, a_1, s_2, a_2, ..., s_t, a_t) ```\n",
        "\n",
        "where:\n",
        "\n",
        "- s_t: current state\n",
        "\n",
        "- a_t: action taken\n",
        "\n",
        "- s_(t+1): next state\n",
        "\n",
        "This equation states that the probability of transitioning to the next state (s_(t+1)) depends only on the current state (s_t) and the action taken (a_t), not on the previous states and actions (s_1, a_1, s_2, a_2, ...)."
      ],
      "metadata": {
        "id": "nUz-uXsSlqVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Limitations**:\n",
        "The most significant limitation of MDPs is the assumption of the Markov property. This means that the future state depends only on the current state and the action taken, not on the past history. In many real-world scenarios, this assumption may not hold true, as past events can influence future outcomes. For instance, in a complex game like chess, the past moves can significantly impact the current state and future possibilities."
      ],
      "metadata": {
        "id": "JrO3KwTqmP7N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0kWHXnVlcKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}