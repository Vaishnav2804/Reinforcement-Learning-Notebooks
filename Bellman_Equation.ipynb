{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF2Q5GVtzUziYXthvcv5Zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishnav2804/Reinforcement-Learning-Notebooks/blob/main/Bellman_Equation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_mg80QKolma"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Bellman Equation:\n",
        " is a fundamental concept in Reinforcement Learning and is closely related to the Markov Decision Process (MDP). While an MDP describes the environment and the decision-making framework, the Bellman Equation helps us understand how to optimally make decisions (i.e., choose actions) by recursively breaking down problems of optimality.\n",
        "\n",
        "# What is the Bellman Equation?\n",
        "\n",
        "The Bellman Equation tells us how to compute the value of a state, which is the expected long-term reward starting from that state and following a particular policy (a strategy for choosing actions). It decomposes the value of a state into two parts:\n",
        "\n",
        "The immediate reward from being in that state.\n",
        "The expected value of the next state (which depends on the chosen action and the transition probabilities).\n",
        "Bellman Equation for State-Value (V-function)\n",
        "For a given policy\n",
        "ðœ‹\n",
        "Ï€, the state-value function\n",
        "ð‘‰\n",
        "ðœ‹\n",
        "(\n",
        "ð‘ \n",
        ")\n",
        "V\n",
        "Ï€\n",
        " (s) is defined as:\n",
        "\n",
        "\n",
        "ð‘‰\n",
        "ðœ‹\n",
        "(\n",
        "ð‘ \n",
        ")\n",
        "=\n",
        "ð¸\n",
        "[\n",
        "ð‘…\n",
        "ð‘¡\n",
        "+\n",
        "ð›¾\n",
        "ð‘‰\n",
        "ðœ‹\n",
        "(\n",
        "ð‘ \n",
        "ð‘¡\n",
        "+\n",
        "1\n",
        ")\n",
        "âˆ£\n",
        "ð‘ \n",
        "ð‘¡\n",
        "=\n",
        "ð‘ \n",
        "]\n",
        "V\n",
        "Ï€\n",
        " (s)=E[R\n",
        "t\n",
        "â€‹\n",
        " +Î³V\n",
        "Ï€\n",
        " (s\n",
        "t+1\n",
        "â€‹\n",
        " )âˆ£s\n",
        "t\n",
        "â€‹\n",
        " =s]\n",
        "\n",
        "Where:\n",
        "\n",
        "ð‘‰\n",
        "ðœ‹\n",
        "(\n",
        "ð‘ \n",
        ")\n",
        "V\n",
        "Ï€\n",
        " (s): Value of state\n",
        "ð‘ \n",
        "s under policy\n",
        "ðœ‹\n",
        "Ï€.\n",
        "\n",
        "ð‘…\n",
        "ð‘¡\n",
        "R\n",
        "t\n",
        "â€‹\n",
        " : Reward received after taking an action at time step\n",
        "ð‘¡\n",
        "t.\n",
        "\n",
        "ð›¾\n",
        "Î³: Discount factor (0 â‰¤\n",
        "ð›¾\n",
        "Î³ â‰¤ 1), which determines the importance of future rewards. If\n",
        "ð›¾\n",
        "Î³ is close to 1, future rewards are considered more important.\n",
        "\n",
        "ð‘ \n",
        "ð‘¡\n",
        "+\n",
        "1\n",
        "s\n",
        "t+1\n",
        "â€‹\n",
        " : The next state after taking an action.\n",
        "\n",
        "This equation is recursive, meaning the value of the current state depends on the value of the next state, and so on.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lCnV0pSloqAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference Between MDP and Bellman Equation MDP:\n",
        "\n",
        "**MDP:** Describes the environment (states, actions, transitions, rewards) and defines how the agent interacts with the environment.\n",
        "\n",
        "**Bellman Equation:** Describes how to compute the value of states and actions by recursively breaking down the problem into immediate reward and future reward. It helps in solving the MDP by finding the value of each state and determining the best policy to follow."
      ],
      "metadata": {
        "id": "8DGAQMObpHMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short, the MDP gives you the structure of the problem, while the Bellman Equation provides the mathematical framework to solve that problem by finding optimal strategies (policies) for decision-making."
      ],
      "metadata": {
        "id": "93C5_ceLpfxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example with Python Code\n",
        "Let's take a simplified gridworld and apply the Bellman Equation. We'll calculate the value of each state based on random rewards and transitions."
      ],
      "metadata": {
        "id": "l--CXwtlpUc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid size and discount factor\n",
        "grid_size = 4\n",
        "gamma = 0.9\n",
        "\n",
        "# Initialize a random reward matrix for each state\n",
        "reward_matrix = np.random.uniform(-1, 1, (grid_size, grid_size))\n",
        "\n",
        "# Initialize the value function for each state as zeros\n",
        "value_function = np.zeros((grid_size, grid_size))\n",
        "\n",
        "# Bellman equation iteration\n",
        "def bellman_update(value_function, reward_matrix, gamma, iterations=10):\n",
        "    for _ in range(iterations):\n",
        "        new_value_function = np.copy(value_function)\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                # For simplicity, assume actions lead to the neighboring states deterministically\n",
        "                # Calculate value based on neighboring states (up, down, left, right)\n",
        "                neighbors = []\n",
        "                if i > 0: neighbors.append(value_function[i-1, j])  # Up\n",
        "                if i < grid_size-1: neighbors.append(value_function[i+1, j])  # Down\n",
        "                if j > 0: neighbors.append(value_function[i, j-1])  # Left\n",
        "                if j < grid_size-1: neighbors.append(value_function[i, j+1])  # Right\n",
        "\n",
        "                # Bellman update: current reward + discounted value of the best neighboring state\n",
        "                best_neighbor_value = max(neighbors) if neighbors else 0\n",
        "                new_value_function[i, j] = reward_matrix[i, j] + gamma * best_neighbor_value\n",
        "\n",
        "        value_function = new_value_function  # Update value function\n",
        "\n",
        "    return value_function\n",
        "\n",
        "# Apply the Bellman equation to update value functions\n",
        "updated_value_function = bellman_update(value_function, reward_matrix, gamma)\n",
        "print(\"Updated Value Function:\")\n",
        "print(updated_value_function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMFC3r7JpF-G",
        "outputId": "2a9f835a-bb3d-405a-ba4e-7b3dde6d6b93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Value Function:\n",
            "[[3.83824622 3.94704488 3.51919715 2.86795924]\n",
            " [3.58631347 3.85965337 2.53612369 3.41383342]\n",
            " [2.81434937 3.80723306 3.70478639 3.20305854]\n",
            " [2.59295105 2.55822991 3.18615782 2.9420762 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ttmXKFnJpWj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "Reward Matrix: We randomly initialize a reward matrix where each state has some reward.\n",
        "Value Function: Initially, we start with a value of zero for all states.\n",
        "Bellman Update: For each state, we look at the neighboring states (UP, DOWN, LEFT, RIGHT) and calculate the updated value using the Bellman equation. The new value is the immediate reward plus the discounted value of the best neighboring state.\n",
        "# Bellman Equation in Action:\n",
        "This code simulates the process of updating the state-value function based on rewards and future expected rewards.\n",
        "The state values gradually update over iterations to reflect the expected long-term reward from each state."
      ],
      "metadata": {
        "id": "MITsJhnGpYlM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5vSHMEwpbqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}